{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d77a7c0-1613-4947-8b77-641adae4bb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer, StringIndexer\n",
    "from pyspark.ml.classification import LinearSVC, OneVsRest\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ccd3fc63-8ccb-4c7d-997c-22a73674d27f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/unamed/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/unamed/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b5ecd52-4768-4e5b-8e98-8770a1adc0d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/05/07 10:56:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/05/07 10:56:32 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 60554)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.12/socketserver.py\", line 318, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/usr/lib/python3.12/socketserver.py\", line 349, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/usr/lib/python3.12/socketserver.py\", line 362, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/usr/lib/python3.12/socketserver.py\", line 761, in __init__\n",
      "    self.handle()\n",
      "  File \"/home/unamed/.virtualenvs/ml/lib/python3.12/site-packages/pyspark/accumulators.py\", line 295, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/home/unamed/.virtualenvs/ml/lib/python3.12/site-packages/pyspark/accumulators.py\", line 267, in poll\n",
      "    if self.rfile in r and func():\n",
      "                           ^^^^^^\n",
      "  File \"/home/unamed/.virtualenvs/ml/lib/python3.12/site-packages/pyspark/accumulators.py\", line 271, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/unamed/.virtualenvs/ml/lib/python3.12/site-packages/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Création de la session Spark\n",
    "spark = SparkSession.builder.appName(\"Twitter Sentiment Analysis\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21d75261-77d6-42d8-b1d1-b5a04bda9c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définir le schéma pour le fichier CSV\n",
    "schema = StructType([\n",
    "    StructField(\"Tweet ID\", IntegerType(), True),\n",
    "    StructField(\"Entity\", StringType(), True),\n",
    "    StructField(\"Sentiment\", StringType(), True),\n",
    "    StructField(\"Tweet content\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb846f2d-9ec6-49ba-bef3-730eaa37e25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement des données depuis le fichier CSV avec le schéma spécifié\n",
    "df = spark.read.option(\"header\", \"true\").schema(schema).csv(\"twitter_training.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5484af9a-1704-4f1b-b086-2ae05c16940d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrage des lignes contenant des valeurs nulles dans la colonne \"Tweet content\"\n",
    "df = df.filter(df[\"Tweet content\"].isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aca1d71b-e884-41d7-af8d-85285fdb315b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Tweet ID: integer (nullable = true)\n",
      " |-- Entity: string (nullable = true)\n",
      " |-- Sentiment: string (nullable = true)\n",
      " |-- Tweet content: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Affichage du schéma pour vérifier le nom des colonnes\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c90d82de-1e63-4e57-8964-697f637fba85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition des étapes de prétraitement et du modèle dans la pipeline\n",
    "tokenizer = Tokenizer(inputCol=\"Tweet content\", outputCol=\"words\")\n",
    "remover = StopWordsRemover(inputCol=tokenizer.getOutputCol(), outputCol=\"filtered_words\")\n",
    "countVectorizer = CountVectorizer(inputCol=remover.getOutputCol(), outputCol=\"features\")\n",
    "labelIndexer = StringIndexer(inputCol=\"Sentiment\", outputCol=\"label\")\n",
    "svm = LinearSVC(maxIter=10)\n",
    "ovr = OneVsRest(classifier=svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01c05119-84c1-4890-8be7-912528947399",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création de la pipeline\n",
    "pipeline = Pipeline(stages=[tokenizer, remover, countVectorizer, labelIndexer, ovr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a4811291-86cd-4171-b01b-3c77e3d09765",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Division des données en ensembles d'entraînement et de test\n",
    "(training_data, test_data) = df.randomSplit([0.8, 0.2], seed=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e1d01d2d-eba1-4732-a902-5fcf7b09aff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/07 10:56:38 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: 2401, Borderlands, Positive, im getting on borderlands and i will murder you all ,\n",
      " Schema: Tweet ID, Entity, Sentiment, Tweet content\n",
      "Expected: Tweet ID but found: 2401\n",
      "CSV file: file:///home/unamed/Projects/MST/bigData/twitter_training.csv\n",
      "24/05/07 10:56:43 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: 2401, Borderlands, Positive, im getting on borderlands and i will murder you all ,\n",
      " Schema: Tweet ID, Entity, Sentiment, Tweet content\n",
      "Expected: Tweet ID but found: 2401\n",
      "CSV file: file:///home/unamed/Projects/MST/bigData/twitter_training.csv\n",
      "24/05/07 10:56:44 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: 2401, Borderlands, Positive, im getting on borderlands and i will murder you all ,\n",
      " Schema: Tweet ID, Entity, Sentiment, Tweet content\n",
      "Expected: Tweet ID but found: 2401\n",
      "CSV file: file:///home/unamed/Projects/MST/bigData/twitter_training.csv\n",
      "24/05/07 10:56:45 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: 2401, Borderlands, Positive, im getting on borderlands and i will murder you all ,\n",
      " Schema: Tweet ID, Entity, Sentiment, Tweet content\n",
      "Expected: Tweet ID but found: 2401\n",
      "CSV file: file:///home/unamed/Projects/MST/bigData/twitter_training.csv\n",
      "24/05/07 10:56:49 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.VectorBLAS\n"
     ]
    }
   ],
   "source": [
    "# Entraînement de la pipeline\n",
    "pipeline_model = pipeline.fit(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "28e46ba9-4cb1-4e10-9052-61af26890440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prédiction sur les données de test\n",
    "predictions = pipeline_model.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d7a367ae-e1ee-46a5-bee9-36310850630c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/07 10:57:06 WARN DAGScheduler: Broadcasting large task binary with size 2.9 MiB\n",
      "24/05/07 10:57:07 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: 2401, Borderlands, Positive, im getting on borderlands and i will murder you all ,\n",
      " Schema: Tweet ID, Entity, Sentiment, Tweet content\n",
      "Expected: Tweet ID but found: 2401\n",
      "CSV file: file:///home/unamed/Projects/MST/bigData/twitter_training.csv\n",
      "[Stage 107:======================================>                  (2 + 1) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of SVM model: 0.8786407766990292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Évaluation du modèle\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol=\"label\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Accuracy of SVM model:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "002740bd-91b3-4e19-80b5-92f33909f84f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tweet ID', 'Entity', 'Sentiment', 'Tweet content', 'words', 'filtered_words', 'features', 'label', 'rawPrediction', 'prediction']\n"
     ]
    }
   ],
   "source": [
    "print(predictions.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04400db4-13a5-4d0a-9ce7-fb64d0cccaaa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
