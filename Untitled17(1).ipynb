{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06924cd8-2ef0-450d-a572-90d719bd9190",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF, VectorAssembler, StringIndexer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "from pyspark.sql.functions import col, when\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8808f26c-5e31-4106-9c84-1595592a410c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/unamed/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Configure NLTK\n",
    "nltk.download('stopwords')\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3bee331f-36b4-48cd-89c3-2a11eaf663ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/05/11 21:59:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Create Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Twitter Sentiment Analysis\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33369e60-fab8-4836-a1d1-a60e8a7e92e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define schema for the CSV file\n",
    "schema = StructType([\n",
    "    StructField(\"Tweet ID\", IntegerType(), True),\n",
    "    StructField(\"Entity\", StringType(), True),\n",
    "    StructField(\"Sentiment\", StringType(), True),\n",
    "    StructField(\"Tweet content\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7bc0fc89-9cc8-443c-b47b-7a42ba83c8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CSV data with specified schema\n",
    "df = spark.read.csv(\"twitter_training.csv\", header=True, schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71b79b82-bb71-4316-9802-a8eba6789dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace null values in the 'Tweet content' column with an empty string\n",
    "df_cleaned = df.withColumn('Tweet content', when(col('Tweet content').isNull(), '').otherwise(col('Tweet content')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87e1abaa-460c-4ddd-bb33-e4e2b651edb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define preprocessing stages\n",
    "tokenizer = Tokenizer(inputCol=\"Tweet content\", outputCol=\"words\")\n",
    "stopwords_remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\")\n",
    "hashing_tf = HashingTF(inputCol=\"filtered_words\", outputCol=\"raw_features\")\n",
    "idf = IDF(inputCol=\"raw_features\", outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "718253bb-b127-452f-a7c0-303c988754a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indexer for Sentiment column (in preprocessing)\n",
    "sentiment_indexer = StringIndexer(inputCol=\"Sentiment\", outputCol=\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f2fdc61-ff5a-4bdd-ad95-10b4717dfaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the preprocessing pipeline (with indexer)\n",
    "preprocessing_pipeline = Pipeline(stages=[tokenizer, stopwords_remover, hashing_tf, idf, sentiment_indexer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "844bcfb5-ad09-45dd-8f7c-cdc5ae7e7849",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/11 22:00:19 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: im getting on borderlands and i will murder you all ,\n",
      " Schema: Tweet content\n",
      "Expected: Tweet content but found: im getting on borderlands and i will murder you all ,\n",
      "CSV file: file:///home/unamed/Projects/MST/bigData/twitter_training.csv\n",
      "24/05/11 22:00:25 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: Positive\n",
      " Schema: Sentiment\n",
      "Expected: Sentiment but found: Positive\n",
      "CSV file: file:///home/unamed/Projects/MST/bigData/twitter_training.csv\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Fit and transform the preprocessing pipeline\n",
    "preprocessed_data = preprocessing_pipeline.fit(df_cleaned).transform(df_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca205ad1-d09b-40b6-a1a0-9fb4033a694e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and test sets\n",
    "train_data, test_data = preprocessed_data.randomSplit([0.8, 0.2], seed=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ef759359-eaeb-4a20-8a81-0aa9130596ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assemble features for the final pipeline (without indexer)\n",
    "final_assembler = VectorAssembler(inputCols=[\"features\"], outputCol=\"final_features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2df1226e-a789-45fc-898e-755e98a058e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression model\n",
    "lr = LogisticRegression(featuresCol='final_features', labelCol='label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "47d3021e-1095-41e9-aef7-8e5bc753c6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the final pipeline (without indexer)\n",
    "final_pipeline = Pipeline(stages=[final_assembler, lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd8a100-a172-484b-8d0f-e54edd70cf4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/11 22:00:32 WARN DAGScheduler: Broadcasting large task binary with size 18.8 MiB\n",
      "24/05/11 22:00:33 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: 2401, Borderlands, Positive, im getting on borderlands and i will murder you all ,\n",
      " Schema: Tweet ID, Entity, Sentiment, Tweet content\n",
      "Expected: Tweet ID but found: 2401\n",
      "CSV file: file:///home/unamed/Projects/MST/bigData/twitter_training.csv\n",
      "24/05/11 22:00:38 WARN DAGScheduler: Broadcasting large task binary with size 18.8 MiB\n",
      "24/05/11 22:00:39 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: 2401, Borderlands, Positive, im getting on borderlands and i will murder you all ,\n",
      " Schema: Tweet ID, Entity, Sentiment, Tweet content\n",
      "Expected: Tweet ID but found: 2401\n",
      "CSV file: file:///home/unamed/Projects/MST/bigData/twitter_training.csv\n",
      "24/05/11 22:00:42 WARN DAGScheduler: Broadcasting large task binary with size 18.8 MiB\n",
      "24/05/11 22:00:44 WARN DAGScheduler: Broadcasting large task binary with size 18.8 MiB\n",
      "24/05/11 22:00:47 WARN DAGScheduler: Broadcasting large task binary with size 18.8 MiB\n",
      "24/05/11 22:00:49 WARN DAGScheduler: Broadcasting large task binary with size 18.8 MiB\n",
      "24/05/11 22:00:51 WARN DAGScheduler: Broadcasting large task binary with size 18.8 MiB\n",
      "24/05/11 22:00:53 WARN DAGScheduler: Broadcasting large task binary with size 18.8 MiB\n",
      "24/05/11 22:00:56 WARN DAGScheduler: Broadcasting large task binary with size 18.8 MiB\n",
      "24/05/11 22:00:58 WARN DAGScheduler: Broadcasting large task binary with size 18.8 MiB\n",
      "24/05/11 22:01:00 WARN DAGScheduler: Broadcasting large task binary with size 18.8 MiB\n",
      "24/05/11 22:01:02 WARN DAGScheduler: Broadcasting large task binary with size 18.8 MiB\n",
      "24/05/11 22:01:04 WARN DAGScheduler: Broadcasting large task binary with size 18.8 MiB\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Train the final pipeline\n",
    "pipeline_model = final_pipeline.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de76cf7-b828-49af-9dba-781bc195bffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Make predictions on the test data\n",
    "predictions = pipeline_model.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c8ed5b-6b85-4002-8282-3eab676764db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create evaluators for accuracy, F1 score, and recall\n",
    "evaluator_accuracy = MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol=\"label\", metricName=\"accuracy\")\n",
    "evaluator_f1 = MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol=\"label\", metricName=\"f1\")\n",
    "evaluator_recall = MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol=\"label\", metricName=\"weightedRecall\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb84ce32-c326-4f0e-a269-e800ba92e34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "accuracy = evaluator_accuracy.evaluate(predictions)\n",
    "f1_score = evaluator_f1.evaluate(predictions)\n",
    "recall = evaluator_recall.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56389b5-b561-43d4-b787-47f85c331131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the results\n",
    "print(\"Accuracy of Logistic Regression model:\", accuracy)\n",
    "print(\"F1 Score of Logistic Regression model:\", f1_score)\n",
    "print(\"Weighted Recall of Logistic Regression model:\", recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41766bc0-4eb4-4e3b-b269-1465288e5bdf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
